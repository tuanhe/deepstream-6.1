################################################################################
# Copyright (c) 2021-2022 NVIDIA Corporation.  All rights reserved.
#
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the "Software"),
# to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
# DEALINGS IN THE SOFTWARE.
################################################################################

This document describes the procedure to run Deepstream samples using Triton
Inference Server through gRPC.

Triton Inference Server will run in a separate deepstream-triton container. Deepstream
nvinferserver gRPC samples will communicate to the server through gRPC.

On dGPU platforms, these samples are meant to be executed inside DeepStream's
Triton Inference Server container.

On Jetson platforms, these samples are meant to be executed on target device
directly or inside DeepStream L4T container.

For DeepStream Triton container setup, please refer to samples/configs/deepstream-app-triton/README
for instructions on pulling the container image and starting Deepstream Triton container.

Inside DS Triton Server container, models and triton_model_repo directory
and prepare_ds_triton_model_repo.sh script from DS SDK is required to setup model
repository for Triton server.

--------------------------------------------------------------------------------
Running the Triton Inference server inside DeepStream Triton Container(Only for X86)
--------------------------------------------------------------------------------
Triton Inference server will run in a separate container. For that DeepStream Triton
docker image from the NGC will be used. Refer to samples/configs/deepstream-app-triton/README
to setup DeepStream Triton server container. Run the container with cmdline:

  $ docker run --gpus '"'device=0'"' -it --rm -v /tmp/.X11-unix:/tmp/.X11-unix \
    -e DISPLAY=$DISPLAY --net=host nvcr.io/nvidia/deepstream:[version]-triton
  [version] means deepstream version numbers. e.g. 6.1

Inside the container, easiest way to set up the model repository with DS provided
models is to run prepare_ds_triton_model_repo.sh script.
Go to samples directory and run the following command.
  $ ./prepare_ds_triton_model_repo.sh

Note:
1. Above step is optional if model repository is already setup.

All the sample models should be downloaded/generated into samples/triton-model-repo
directory.
Please refer to  https://github.com/triton-inference-server/server/blob/main/docs/model_repository.md for more details on creating model repositories.

Once the model repository is ready, Goto folder: /opt/nvidia/deepstream/deepstream/samples,
run the following command to start the Triton Inference Server.

  $ tritonserver --model-repository=triton_model_repo

Note:
1. Provide the actual path of model repository if it is different from the above
   mentioned path.
2. Docker container for Triton server is only for x86 machine. To run Triton
   server on Jetson device refer next section.

Please refer to https://github.com/triton-inference-server/server/blob/main/docs/quickstart.md for more information on running the Triton Inference Server.

--------------------------------------------------------------------------------
Setting up Triton Inference Server backends (Only for Jetson)
--------------------------------------------------------------------------------
Deepstream Triton container image has Triton Inference Server and supported
backend libraries pre-installed. But in case of Jetson to run the Triton
Inference Server direclty on device, Triton Server setup will be required.
1. Go to samples directory and run the following command to set up the Triton
   Server and backends.
   $ sudo ./triton_backend_setup.sh

Note:
1. By default script will download the Triton Server version 2.20.0. For setting
   up any other version change the package path accordingly.
2. By default script removes the downloaded package after copying the backend
   binaries. In case you want to run both the Triton Server and DS Apps on the
   same Jetson device, disable that step in script and use the following steps
   to start the Triton server.
   $ cd /tmp/triton_server_downloads/bin
   $ ./tritonserver --model-repository=/opt/nvidia/deepstream/deepstream/samples/triton_model_repo/ --backend-directory=/opt/nvidia/deepstream/deepstream/lib/triton_backends/

--------------------------------------------------------------------------------
Running the Triton Inference Server gRPC Client samples
--------------------------------------------------------------------------------
For X86, the gRPC samples must run in another DeepStream Triton container.
Start another container:

$ docker run --gpus '"'device=0'"' -it --rm -v /tmp/.X11-unix:/tmp/.X11-unix \
    -e DISPLAY=$DISPLAY --net=host nvcr.io/nvidia/deepstream:[version]-triton
  [version] means deepstream version numbers. e.g. 6.1

For Jetson, the gRPC samples could run on L4T host directly.

Once the DeepStream Triton environment is ready, run the following commands:
1. Run the following command to inspect nvinferserver plugin installed successfully.
   $ gst-inspect-1.0 nvinferserver
2. In case Triton Inference Server is running on separate host machine, modify
   the nvinferserver configuration files (e.g config_infer_plan_engine_primary.txt)
   under configs/deepstream-app-triton-grpc to update gRPC url. By default it is
   set to "localhost:8001"
3. Run the following command to start the app.
   $ deepstream-app -c <path to source....txt>
     e.g. $ deepstream-app -c source30_1080p_dec_infer-resnet_tiled_display_int8.txt
3. Application config files included in `configs/deepstream-app-triton-grpc/`
   a. source30_1080p_dec_infer-resnet_tiled_display_int8.txt (30 Decode + Infer)
   b. source4_1080p_dec_infer-resnet_tracker_sgie_tiled_display_int8.txt
      (4 Decode + Infer + SGIE + Tracker)
   In some cases, model inference might not be in realtime and frames get
   dropped, in that case, edit source...txt file and update 'sync=0' in [sink..] group.
4. Configuration files for "nvinferserver" element in `configs/deepstream-app-triton/`
   a. config_infer_plan_engine_primary.txt (Primary Object Detector)
   b. config_infer_secondary_plan_engine_carcolor.txt (Secondary Car Color Classifier)
   c. config_infer_secondary_plan_engine_carmake.txt (Secondary Car Make Classifier)
   d. config_infer_secondary_plan_engine_vehicletypes.txt (Secondary Vehicle Type Classifier)

--------------------------------------------------------------------------------
Notes:
--------------------------------------------------------------------------------
1. If the application runs into errors, cannot create gst elements, try again
after removing gstreamer cache
   rm ${HOME}/.cache/gstreamer-1.0/registry.x86_64.bin
2. When running deepstream for first time, the following warning might show up:
"GStreamer-WARNING: Failed to load plugin '...libnvdsgst_inferserver.so':
libtrtserver.so: cannot open shared object file: No such file or directory"
This is a harmless warning indicating that the DeepStream's nvinferserver plugin
cannot be used since "Triton Inference Server" is not installed.
If required, try DeepStream's Triton docker image or install the Triton Inference
Server manually. For more details, refer to https://github.com/NVIDIA/triton-inference-server.
