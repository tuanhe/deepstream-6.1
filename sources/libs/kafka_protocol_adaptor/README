################################################################################
# Copyright (c) 2018-2021, NVIDIA CORPORATION.  All rights reserved.
#
# NVIDIA Corporation and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA Corporation is strictly prohibited.
#
################################################################################
 
This project implements protocol adaptor for kafka.
The adaptor implements and exposed the DSMI API for client applications to interface with it.

Dependencies
-------------
Build dependencies with installation instructions:
* librdkafka
  Note that for using TLS/SSL security, make sure to build librdkafka with
  SSL suport enabled by using the enable_ssl option while running
  'configure', as shown below.

  git clone https://github.com/edenhill/librdkafka.git
  cd librdkafka
  git reset --hard 063a9ae7a65cebdf1cc128da9815c05f91a2a996
  ./configure --enable-ssl
  make
  sudo make install
  sudo cp /usr/local/lib/librdkafka* /opt/nvidia/deepstream/deepstream/lib/
  sudo ldconfig

NOTE: To compile the sources, run make with "sudo" or root permission.

* glib 2.0

  apt-get install libglib2.0 libglib2.0-dev

* jansson

  apt-get install  libjansson4  libjansson-dev

* ssl

  apt-get install libssl-dev

(optional) : Install & setup kafka broker on your machine & create topic(s)
--------------------------------------------------------------------------
Follow instructions here: https://kafka.apache.org/quickstart

Kafka cfg:
----------
You can add Kafka configuration and connection related details in cfg_kafka.txt
Uncomment the fields you may want to edit and add proper values

example:
[message-broker]
consumer-group-id = mygrp
proto-cfg = "message.max.bytes=200000;log_level=6"
producer-proto-cfg = "queue.buffering.max.messages=200000;message.send.max.retries=3"
consumer-proto-cfg = "max.poll.interval.ms=20000"
partition-key = mykey
#share-connection = 1

Uncomment the field share-connection and set it to value 1 if you need to
generate a connection signature. This signature is a unique string which is
generated by parsing all the kafka connection related params
used for making a connection
Uncommenting this field signifies that the connection created can be shared
with other components within the same process.

NOTE:
1. Kafka connection string (host;port) must be specified in call to connect
    ex: nvds_msgapi_connect("hostname;port" , (nvds_msgapi_connect_cb_t) sample_msgapi_connect_cb, (char *)CFG_FILE);
2. Subscribe api should be called just once, per connection handle
   Any new subscribe() call will override the already existing topic subscription with the newer topics provided
3. Publish fails if send topic is non-existent (i.e topic not created in kafka broker or if auto topic generation not enabled in broker)
4. Consumer silently fails if subscribe topic is non-existent (i.e topic not created in kafka broker or if auto topic generation not enabled in broker)
   https://github.com/edenhill/librdkafka/issues/1201
5. Default values used:
   #define DEFAULT_KAFKA_CONSUMER_GROUP "test-consumer-group"
   #define DEFAULT_PARTITION_NAME "sensor.id"


Building the adaptor
---------------------
Upon installing the dependencies, to build adaptor library execute 'make'.

Refer to test program for illustration of using the adaptor.

Sample programs
------------------
To build test program execute 'make -f Makefile.test'

Make sure to modify the address for the kafka broker being connected to as part of the call to msgapi_connect_ptr:

  conn_handle = msgapi_connect_ptr((char *)"yourserver.yourdomain.net;9092",(nvds_msgapi_connect_cb_t) sample_msgapi_connect_cb, (char *)CFG_FILE);

Subscribing to topic(s)
example:
  const char *topics[] = {"yourtopic1", "yourtopic2"};
  int num_topics = 2;
  msgapi_subscribe_ptr(conn_handle, (char **)topics, num_topics, subscribe_cb, &consumed_cnt);

Publishing on a topic
example:
  msgapi_send_ptr(conn_handle, (char *)topic, (const uint8_t*) SEND_MSG, strlen(SEND_MSG));

Before running the sample applications, enable logs by running the logger setup script:
For x86,
 chmod u+x ~/deepstream_x86_public/sources/tools/nvds_logger/setup_nvds_logger.sh
 sudo ~/deepstream_x86_public/sources/tools/nvds_logger/setup_nvds_logger.sh
On Jetson,
 chmod u+x ~/deepstream_sdk_on_jetson/sources/tools/nvds_logger/setup_nvds_logger.sh
 sudo ~/deepstream_sdk_on_jetson/sources/tools/nvds_logger/setup_nvds_logger.sh

Note that for complete set of logs, set the logger level to 7 (DEBUG), as described in the logger README.

To run test program:
  ./test_sample_proto_sync
  ./test_sample_proto_async

Note that the send operation inspects the incoming JSON formatted message to look for a sensor.id field.
This field (if present) is used as message key while sending to kafka broker.
If the key is not present then the default partitioner is used.

Refer to the user guide for adaptor usage information including adaptor API, and configuration options.

Running test program with TLS/SSL (security enabled):
------------------------------------------------------
To enable TLS based authentication and encryption, the steps below need
to be completed. Note that TLS has supplanted SSL, through the terms are
used interchangeably in literature.

* kafka broker needs to be setup with TLS authentication enabled
* create and deploy certificates for broker and deepstream application
* copy client CA certificates to the broker truststore
* configure TLS options in kafka config file (see CFG_FILE param above) used by
  test application

Please refer to the "Security_Setup.md" document in this folder for detailed instructions on completing these steps.

The command to run the test application remains unchanged.
