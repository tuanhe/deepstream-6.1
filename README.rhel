*****************************************************************************
* Copyright (c) 2020-2022 NVIDIA Corporation.  All rights reserved.
*
* NVIDIA Corporation and its licensors retain all intellectual property
* and proprietary rights in and to this software, related documentation
* and any modifications thereto.  Any use, reproduction, disclosure or
* distribution of this software and related documentation without an express
* license agreement from NVIDIA Corporation is strictly prohibited.
*****************************************************************************

================================================================================
DeepStream SDK
================================================================================
Refer to Quickstart Guide (dGPU Setup for RedHat Enterprise Linux (RHEL)) section
of DeepStreamSDK documentation for setup pre-requisites.

--------------------------------------------------------------------------------
Package Contents
--------------------------------------------------------------------------------
The DeepStream packages include:
1. sources - Sources for sample application and plugin
2. samples - Config files, Models, streams and tools to run the sample app

Note for running with docker
-----------------------------
While running DeepStream with docker, necessary packages are already pre-installed.
Hence please skip the installation steps and proceed to "Running the samples" section of this document.

--------------------------------------------------------------------------------
Installing Pre-requisites:
--------------------------------------------------------------------------------
Packages to be installed with root privileges:
$ yum install \
    gstreamer1 \
    gstreamer1-plugins-base \
    gstreamer1-plugins-good \
    gstreamer1-plugins-bad-free \
    gstreamer1-plugins-ugly-free \
    gstreamer1-svt-av1 \
    json-glib \
    openssl \
    libuuid

NOTE: Refer Quickstart Guide (dGPU Setup for RedHat Enterprise Linux (RHEL)) of
DeepStream Documentation for information on installing CUDA 11.4 and TensorRT 8.0.1.

-------------------------------------------------------------------------------
Installing Pre-requisites to compile the sources:
-------------------------------------------------------------------------------
Packages to be installed with root privileges:

$ yum install \
    gstreamer1-plugins-base-devel \
    json-glib-devel \
    opencv-devel \
    jansson-devel \
    openssl-devel \
    libuuid-devel \
    libX11-devel \
    mesa-libEGL-devel \
    yaml-cpp-devel

gst-rtsp-server
---------------
gst-rtsp-server-devel package is not available which is required to compile deepstream-app
Download sources from https://gstreamer.freedesktop.org/src/gst-rtsp-server/gst-rtsp-server-1.14.5.tar.xz
$ ./configure
$ make
$ make install
$ sudo cp -r /usr/local/include/gstreamer-1.0/gst/rtsp-server/ /usr/include/gstreamer-1.0/gst/
$ sudo cp /usr/local/lib/libgstrtspserver-1.0.so /usr/local/lib/libgstrtspserver-1.0.so.0 \
      /usr/local/lib/libgstrtspserver-1.0.so.0.1601.0 /usr/lib64/

gstreamer1-libav package
------------------------
gst-libav plugins needed for using avdec plugins
Download sources from https://gstreamer.freedesktop.org/src/gst-libav/gst-libav-1.14.5.tar.xz
$ ./configure
$ make
$ make install
$ sudo cp /usr/local/lib/gstreamer-1.0/* /usr/lib64/gstreamer-1.0/

Opencv-Python package for python binding apps
---------------------------------------------
$ sudo pip3 install --upgrade pip
$ pip install opencv-python

x264enc
-------
First install x264 package from videoLAN
$ git clone https://code.videolan.org/videolan/x264.git
$ cd x264
$ ./configure --enable-static --enable-shared
$ make
$ sudo make install

$ sudo vim /etc/ld.so.conf
Add following line to the file --
/usr/local/lib
$ sudo ldconfig

Download sources from https://gstreamer.freedesktop.org/src/gst-plugins-ugly/gst-plugins-ugly-1.14.5.tar.xz
$ ./configure
$ cd ext/x264/
$ export LD_PRELOAD=/usr/local/lib/libx264.so
$ make
$ sudo make install
$ sudo cp /usr/local/lib/gstreamer-1.0/* /usr/lib64/gstreamer-1.0/

-------------------------------------------------------------------------------
Using Kafka protocol adaptor with message broker
-------------------------------------------------------------------------------
Refer README file availabe under sources/libs/kafka_protocol_adaptor for
detailed documentation on prerequisites and usages of kafka protocol
adaptor with message broker plugin for sending messages to cloud.

Refer source code and README of deepstream-test4 available under
sources/apps/sample_apps/deepstream-test4/ to send messages to the cloud.

-----------------------------------------------------------------------
Using azure MQTT protocol adaptor with message broker
-----------------------------------------------------------------------
Refer README files availabe under sources/libs/azure_protocol_adaptor for
detailed documentation on prerequisites and usages of azure MQTT protocol
adaptor with message broker plugin for sending messages to cloud.

Refer source code and README of deepstream-test4 available under
sources/apps/sample_apps/deepstream-test4/ to send messages to the cloud.

--------------------------------------------------------------------------------
Extract and Install DeepStream SDK
--------------------------------------------------------------------------------
1. Untar deepstream_sdk_v6.1.0_x86_64.tbz2
   sudo tar -xvf deepstream_sdk_v6.1.0_x86_64.tbz2 -C /
2. The samples, sources and install script will be found in:
   /opt/nvidia/deepstream/deepstream-6.1/
3. Run the install.sh script as follows:
   sudo ./install.sh

--------------------------------------------------------------------------------
Running the samples
--------------------------------------------------------------------------------
1. Go to samples directory and run
   deepstream-app -c <path to config.txt>
2. Application config files included in `configs/deepstream-app/`
   a. source30_1080p_dec_infer-resnet_tiled_display_int8.txt (30 Decode + Infer)
   b. source4_1080p_dec_infer-resnet_tracker_sgie_tiled_display_int8.txt
      (4 Decode + Infer + SGIE + Tracker)
   c. source4_1080p_dec_infer-resnet_tracker_sgie_tiled_display_int8_gpu1.txt
      (4 Decode + Infer + SGIE + Tracker executed on gpu1)
3. Configuration files for "nvinfer" element in `configs/deepstream-app/`
   a. config_infer_primary.txt (Primary Object Detector)
   b. config_infer_secondary_carcolor.txt (Secondary Car Color Classifier)
   c. config_infer_secondary_carmake.txt (Secondary Car Make Classifier)
   d. config_infer_secondary_vehicletypes.txt (Secondary Vehicle Type Classifier)

--------------------------------------------------------------------------------
Running the TensorRT Inference Server samples
--------------------------------------------------------------------------------
These samples are meant to be executed inside DeepStream's TensorRT Inference
Server container. Refer to the DeepStream Quick Start Guide for instructions
on pulling the container image and starting the container. Once inside the
container, run the following commands:

1. Go to the samples directory and run the following command to prepare the
   model repository.
   $ ./prepare_ds_triton_model_repo.sh
2. Install ffmpeg. It is a pre-requisite to run the next step.
   $ sudo apt-get update && sudo apt-get install ffmpeg
3. Run the following script to create the sample classification video.
   $ ./prepare_classification_test_video.sh
4. Run the following command to start the app.
   $ deepstream-app -c <path to config.txt>
5. Application config files included in `configs/deepstream-app-triton/`
   a. source30_1080p_dec_infer-resnet_tiled_display_int8.txt (30 Decode + Infer)
   b. source4_1080p_dec_infer-resnet_tracker_sgie_tiled_display_int8.txt
      (4 Decode + Infer + SGIE + Tracker)
   c. source1_primary_classifier.txt (Single source + full frame classification)
      NOTE: Other classification models can be used by changing the nvinferserver
            config file in the [*-gie] group of application config file.
   d. source1_primary_detector.txt (Single source + object detection using ssd)
6. Configuration files for "nvinferserver" element in `configs/deepstream-app-triton/`
   a. config_infer_plan_engine_primary.txt (Primary Object Detector)
   b. config_infer_secondary_plan_engine_carcolor.txt (Secondary Car Color Classifier)
   c. config_infer_secondary_plan_engine_carmake.txt (Secondary Car Make Classifier)
   d. config_infer_secondary_plan_engine_vehicletypes.txt (Secondary Vehicle Type Classifier)
   e. config_infer_primary_classifier_densenet_onnx.txt (DenseNet-121 v1.2 classifier)
   f. config_infer_primary_classifier_inception_graphdef_postprocessInTriton.txt
      (Tensorflow Inception v3 classifier - Post processing in TRT-IS)
   g. config_infer_primary_classifier_inception_graphdef_postprocessInDS.txt
      (Tensorflow Inception v3 classifier - Post processing in DeepStream)
   h. config_infer_primary_detector_ssd_inception_v2_coco_2018_01_28.txt
      (TensorFlow SSD Inception V2 Object Detector)

--------------------------------------------------------------------------------
Downloading and Running the Pre-trained Transfer Learning Toolkit Models
--------------------------------------------------------------------------------
Instructions to download and run the pre-trained Transfer Learning Toolkit models
are provided in samples/configs/tao_pretrained_models/README.

--------------------------------------------------------------------------------
Notes:
--------------------------------------------------------------------------------
1. When running TensorFlow models using TensorRT Inference Server, the GPU device
memory may fall short. The allowed GPU device memory allocation for TensorFlow
models can be tuned using the 'tf_gpu_memory_fraction' parameter in the
nvdsinferserver's config files (config_infer_*).
2. When running deepstream for first time, the following warning might show up:
   "GStreamer-WARNING: Failed to load plugin '...libnvdsgst_inferserver.so':
    libtrtserver.so: cannot open shared object file: No such file or directory"
This is a harmless warning indicating that the DeepStream's nvinferserver plugin
cannot be used since "Triton Inference Server" is not installed.
If required, try DeepStream's TRT-IS docker image or install the Triton Inference
Server manually. For more details, refer to https://github.com/NVIDIA/triton-inference-server.
